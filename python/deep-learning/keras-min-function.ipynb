{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to learn the minimum function with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghiatt4/anaconda3/envs/cv36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(m = 10000, max_X = 10000):\n",
    "    \"\"\"\n",
    "    m: number of data entries\n",
    "    max_X: data is constrained in [-max_X, max_X)\n",
    "    \"\"\"\n",
    "    x = np.random.randint(-max_X, max_X, (m, 2))\n",
    "    y = np.argmin(x, axis=1)\n",
    "    y = np.eye(2)[y]  # One hot encoding\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset()\n",
    "X_val, Y_val = create_dataset(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    X_input = Input(batch_shape=(None, 2))\n",
    "    \n",
    "    X = Dense(3)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Dense(3)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Dense(2)(X)\n",
    "    X = Activation('sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "optimizer = Adam(lr=0.03, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.7515 - acc: 0.6061\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.7128 - acc: 0.6349\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.6752 - acc: 0.6680\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.6387 - acc: 0.7020\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.6040 - acc: 0.7376\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.5715 - acc: 0.7612\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.5419 - acc: 0.7611\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.5143 - acc: 0.7749\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.4863 - acc: 0.8046\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.4566 - acc: 0.8439\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.4260 - acc: 0.8863\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.3965 - acc: 0.9354\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.3707 - acc: 0.9702\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.3502 - acc: 0.9707\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.3337 - acc: 0.9489\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.3180 - acc: 0.9409\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.3008 - acc: 0.9415\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.2815 - acc: 0.9484\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.2609 - acc: 0.9606\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.2411 - acc: 0.9793\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.2250 - acc: 0.9919\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.2146 - acc: 0.9809\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.2065 - acc: 0.9701\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1955 - acc: 0.9701\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1814 - acc: 0.9768\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1670 - acc: 0.9890\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1559 - acc: 0.9940\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1489 - acc: 0.9855\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1434 - acc: 0.9795\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1360 - acc: 0.9798\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1266 - acc: 0.9856\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1177 - acc: 0.9941\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1120 - acc: 0.9955\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1088 - acc: 0.9880\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.1045 - acc: 0.9865\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0982 - acc: 0.9905\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0919 - acc: 0.9977\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0881 - acc: 0.9937\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0859 - acc: 0.9891\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0827 - acc: 0.9890\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0780 - acc: 0.9929\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0742 - acc: 0.9980\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0722 - acc: 0.9951\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0705 - acc: 0.9929\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0673 - acc: 0.9948\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0641 - acc: 0.9986\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0624 - acc: 0.9949\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0611 - acc: 0.9926\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0588 - acc: 0.9940\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0563 - acc: 0.9980\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0551 - acc: 0.9973\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0540 - acc: 0.9959\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0521 - acc: 0.9972\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0503 - acc: 0.9985\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0495 - acc: 0.9959\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0484 - acc: 0.9950\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0468 - acc: 0.9973\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0457 - acc: 0.9984\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0450 - acc: 0.9972\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0438 - acc: 0.9980\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0426 - acc: 0.9986\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0420 - acc: 0.9973\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0411 - acc: 0.9973\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0400 - acc: 0.9987\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0394 - acc: 0.9987\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0387 - acc: 0.9981\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0378 - acc: 0.9988\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0372 - acc: 0.9979\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0366 - acc: 0.9976\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0358 - acc: 0.9987\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0353 - acc: 0.9988\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0347 - acc: 0.9987\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0340 - acc: 0.9991\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0336 - acc: 0.9983\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0331 - acc: 0.9980\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0325 - acc: 0.9991\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0320 - acc: 0.9990\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0315 - acc: 0.9991\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0310 - acc: 0.9989\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0306 - acc: 0.9984\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0302 - acc: 0.9988\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0297 - acc: 0.9992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0294 - acc: 0.9992\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0289 - acc: 0.9991\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0286 - acc: 0.9987\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0282 - acc: 0.9988\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0278 - acc: 0.9992\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0275 - acc: 0.9995\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0271 - acc: 0.9992\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0268 - acc: 0.9988\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0264 - acc: 0.9989\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0261 - acc: 0.9993\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0258 - acc: 0.9993\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0255 - acc: 0.9992\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0252 - acc: 0.9988\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0249 - acc: 0.9992\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0246 - acc: 0.9994\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0243 - acc: 0.9992\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0240 - acc: 0.9990\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 1us/step - loss: 0.0238 - acc: 0.9991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3737593128>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=100, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f37320a3128>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt8VPWd//HXZ2YykzsEcoMkXERuQQEh4n2xXiroVrT1\ngvXS33b788fu2nb7292u/vrYPn7d7m+3t3XXtlrrWu1u7Za6aBWtihbvuioBEbkTuUiAQCIQQsht\nku/vjxnpmAYywCQnc+b9fDzmkTnnfJn5fB/A+5x8zznfY845RETEXwJeFyAiIqmncBcR8SGFu4iI\nDyncRUR8SOEuIuJDCncRER9SuIuI+JDCXUTEhxTuIiI+FPLqi4uLi924ceO8+noRkbS0cuXKJudc\nSX/tPAv3cePGUVtb69XXi4ikJTPbkUw7DcuIiPiQwl1ExIcU7iIiPqRwFxHxIYW7iIgPKdxFRHxI\n4S4i4kNpF+6b97bw7afX0xHt9roUEZEhK+3Cvf7AEX72+jbe3rrf61JERIastAv3804rJhIK8OLG\nfV6XIiIyZKVduOeEg5w/YSQvbdqHc87rckREhqS0C3eAS6aUsuOjI2xtavW6FBGRISktw/1TU0oB\neElDMyIifUrLcK8symVSWb7G3UVEjiEtwx1iR+/vbNtPS3uX16WIiAw5aRvul0wuJdrjeH1Lk9el\niIgMOWkb7rPHFlGYHeKlTRqaERHpLW3DPRQM8EeTSnhpUyM9PbokUkQkUdqGO8QuiWxs6WDt7mav\nSxERGVKSCnczm2dmm8yszszu7GP735jZ6vhrrZl1m9mI1Jf7SRdPLsUMlm/Q0IyISKJ+w93MgsC9\nwHygGrjJzKoT2zjnvu+cm+mcmwncBbzinBvwyV9G5IWZNaaI5Rv3DvRXiYiklWSO3OcAdc65rc65\nTmAxsOA47W8CfpWK4pJx6dRS1u46RENz+2B9pYjIkJdMuFcAOxOW6+Pr/oCZ5QLzgMeOsf12M6s1\ns9rGxsYTrbVPl00tA9DRu4hIglSfUP0M8MaxhmSccw8452qcczUlJSUp+cKJpflUjcjRuLuISIJk\nwn0XUJWwXBlf15eFDOKQDICZcemUMt6oa6KtUw/wEBGB5MJ9BTDRzMabWZhYgC/t3cjMhgFzgSdT\nW2L/LptaRke0h9frdLeqiAgkEe7OuShwB7AM2AA86pxbZ2aLzGxRQtNrgeedc4M+D++c8SMoiIR4\nUePuIiIAhJJp5Jx7Bnim17r7ey3/HPh5qgo7EeFQ7G7V5Rv20dPjCATMizJERIaMtL5DNdGlU0vZ\np7tVRUQAH4X7pyaXEjB4Yb2GZkREfBPuRXlhzh43QuEuIoKPwh3g8uoyNja08OFHR7wuRUTEU74K\n909XlwPw/PoGjysREfGWr8J9zMhcppQX8LyGZkQkw/kq3AE+XV1G7fb97G/t9LoUERHP+C7cL68u\np8fB8g06eheRzOW7cD+jopBRw7J11YyIZDTfhbuZcXl1Ga9uadREYiKSsXwX7hC7aqa9q4fXtqRm\nzngRkXTjy3A/57QRDMvJ4rl1uiRSRDKTL8M9KxjgsqllvLB+L53RHq/LEREZdL4Md4D5Z5TT0h7l\nzQ80x7uIZB7fhvuFE4vJj4R4bq2GZkQk8/g23LOzglwypZRl6xqIdmtoRkQyi2/DHeDKM8s5cKSL\nd7b1+bxuERHf8nW4z51USk5WkGc1NCMiGSapcDezeWa2yczqzOzOY7S52MxWm9k6M3sltWWenJxw\nkIsnl/DcugZ6epzX5YiIDJp+w93MgsC9wHygGrjJzKp7tRkO3Adc7ZybBlw/ALWelPlnjqKxpYOV\nHx7wuhQRkUGTzJH7HKDOObfVOdcJLAYW9GrzeeBx59yHAM65fakt8+RdMqWUcCjAb9fs8boUEZFB\nk0y4VwA7E5br4+sSTQKKzOxlM1tpZrf19UFmdruZ1ZpZbWPj4EwNkB8JcemUUp5es0dXzYhIxkjV\nCdUQMBu4CrgC+Dszm9S7kXPuAedcjXOupqSkJEVf3b+rZ4ym6XAH/731o0H7ThERLyUT7ruAqoTl\nyvi6RPXAMudcq3OuCXgVmJGaEk/dp6aUUhAJsXT1bq9LEREZFMmE+wpgopmNN7MwsBBY2qvNk8CF\nZhYys1zgHGBDaks9edlZQT49rZzn1jXQ3qVpgEXE//oNd+dcFLgDWEYssB91zq0zs0VmtijeZgPw\nHLAGeAd40Dm3duDKPnFXzxxNS3uUlzdpGmAR8b9QMo2cc88Az/Rad3+v5e8D309daal1wYSRjMwL\n89R7u5l3RrnX5YiIDChf36GaKBQMcNX0Ufxuw14Od0S9LkdEZEBlTLhD7KqZjmgPz+shHiLicxkV\n7rPGFFFZlMPjq3pf7CMi4i8ZFe6BgHH97Cre+KCJnfuPeF2OiMiAyahwB/jc7NjNtY+tqve4EhGR\ngZNx4V5ZlMsFE4pZsrJeM0WKiG9lXLgDXF9TSf2BNt7SdAQi4lMZGe5XTCunMDvEo7U7+28sIpKG\nMjLcs7OCLJhZwbNrG2hu6/K6HBGRlMvIcIfY0ExHtIel72kyMRHxn4wN9zMrhjGlvIDF73yIczqx\nKiL+krHhbmbcfO5Y1u0+xOqdB70uR0QkpTI23AGuPauCvHCQR9760OtSRERSKqPDPT8S4pqzKnh6\nzW4OHun0uhwRkZTJ6HAHuOXcsXREe1iyUnesioh/ZHy4Tx1VyOyxRfzy7Q91x6qI+EbGhzvAreeO\nZVtTK29+oDtWRcQfFO7A/DPLGZEX5pG3dnhdiohISiQV7mY2z8w2mVmdmd3Zx/aLzazZzFbHX99M\nfakDJxIKcv3sSl7YsJe9h9q9LkdE5JT1G+5mFgTuBeYD1cBNZlbdR9PXnHMz46+/T3GdA+6mOWPo\n7nE8ukLzzYhI+kvmyH0OUOec2+qc6wQWAwsGtqzBN644jwtPL2bxip1068SqiKS5ZMK9Akg8nK2P\nr+vtfDNbY2bPmtm0lFQ3yG4+Zwy7DrbxyuZ9XpciInJKUnVCdRUwxjk3HfgR8ERfjczsdjOrNbPa\nxsbGFH116lxWXUZJQYT/fFt3rIpIeksm3HcBVQnLlfF1RznnDjnnDsffPwNkmVlx7w9yzj3gnKtx\nztWUlJScQtkDIysY4MaaKl7cuI/dB9u8LkdE5KQlE+4rgIlmNt7MwsBCYGliAzMrNzOLv58T/9y0\nvGh84ZwqHLBYJ1ZFJI31G+7OuShwB7AM2AA86pxbZ2aLzGxRvNl1wFozew/4IbDQpek8upVFucyd\nVMKjOrEqImkslEyj+FDLM73W3Z/w/sfAj1NbmnduqKniz3+5itfrmpg7aegNH4mI9Ed3qPbh0qml\nFOVm6RmrIpK2FO59iIRiz1h9Yd1eTQUsImlJ4X4MN9RU0dndw5Or9YxVEUk/CvdjqB5dyLTRhfzX\nSg3NiEj6Ubgfxw01VazddYj1uw95XYqIyAlRuB/HgpmjCQcDOnoXkbSjcD+O4blhLp9WxhPv7qIz\n2uN1OSIiSVO49+O6WZUcONLFy5s0mZiIpA+Fez8umlhMcX6Ex1bpAdoikj4U7v0IBQNcM3M0L27c\nx4FWXfMuIulB4Z6Ez86qpKvb8dQaXfMuIulB4Z6E6tGFTB1VyGMrNTQjIulB4Z6kz82q4L36Zur2\ntXhdiohIvxTuSVows4JgwHhs1a7+G4uIeEzhnqSSgghzJ5Xwm1W7NM+7iAx5CvcTcP3sShoOtfPa\nlqH3/FcRkUQK9xNw6dQyRuSF+a9anVgVkaFN4X4CwqEA18ys4Pn1DezXNe8iMoQlFe5mNs/MNplZ\nnZndeZx2Z5tZ1MyuS12JQ8uNZ1fR1e144l2dWBWRoavfcDezIHAvMB+oBm4ys+pjtPsu8HyqixxK\nJpcXMKNyGI/W7iRNnwEuIhkgmSP3OUCdc26rc64TWAws6KPdl4HHAN/PsHV9TRUbG1p4f1ez16WI\niPQpmXCvABInNK+PrzvKzCqAa4GfpK60oevqmaOJhAJ6gLaIDFmpOqH6r8DfOueOO+m5md1uZrVm\nVtvYmL6XExZmZ3HlmaN4cvVuWjuiXpcjIvIHkgn3XUBVwnJlfF2iGmCxmW0HrgPuM7Nren+Qc+4B\n51yNc66mpKTkJEseGm45dywt7VEe11TAIjIEJRPuK4CJZjbezMLAQmBpYgPn3Hjn3Djn3DhgCfDn\nzrknUl7tEDJrzHBmVA3n4Te206M7VkVkiOk33J1zUeAOYBmwAXjUObfOzBaZ2aKBLnCoMjO+eME4\ntja18vJm359DFpE0Y15dzldTU+Nqa2s9+e5U6eru4aLvvsTppfk88qVzvC5HRDKAma10ztX01053\nqJ6CrGCA284fy+t1TWxq0FTAIjJ0KNxP0efnjCE7K8BDr2/zuhQRkaMU7qdoeG6Yz82q5Derd9HQ\n3O51OSIigMI9JRbNnQAO/vn5TV6XIiICKNxTompELredN5Ylq+rZsOeQ1+WIiCjcU+WOS06nMDuL\nf3p2o9eliIgo3FNleG6YL19yOq9ubuTVzek7tYKI+IPCPYVuPW8sVSNy+MdnNug5qyLiKYV7CkVC\nQe6cN5WNDS3822tbvS5HRDKYwj3FrjyznHnTyrn7+c1sbNDJVRHxhsI9xcyM/3ftGRTmhPjar9+j\nM3rcWZBFRAaEwn0AjMyP8E+fnc6GPYe4Z/lmr8sRkQykcB8gl1eXcf3sSn7y8gfUbt/vdTkikmEU\n7gPom5+ppqIoh68uXs2h9i6vyxGRDKJwH0AF2Vncs/AsGg61843frMWr6ZVFJPMo3AfYrDFF/O/L\nJ/HUe7t5bFXvpxOKiAwMhfsgWDR3AueeNoJvPrmW7U2tXpcjIhlA4T4IggHjX26cSTBgfP2xNXrm\nqogMuKTC3czmmdkmM6szszv72L7AzNaY2WozqzWzC1NfanobNSyHv7uqmne27eeXb+/wuhwR8bl+\nw93MgsC9wHygGrjJzKp7NVsOzHDOzQS+CDyY6kL94PqaSi6aWMw/PbuRnfuPeF2OiPhYMkfuc4A6\n59xW51wnsBhYkNjAOXfY/f5SkDxA4w59MDO+87npGHDX4+/r6hkRGTDJhHsFsDNhuT6+7hPM7Foz\n2wj8ltjRu/ShYngOd105ldfrmliyst7rckTEp1J2QtU59xvn3BTgGuDbfbUxs9vjY/K1jY2ZO+f5\n5+eMYdaY4Xz3uU26uUlEBkQy4b4LqEpYroyv65Nz7lXgNDMr7mPbA865GudcTUlJyQkX6xeBgPF/\nr57GR60d/Gj5Fq/LEREfSibcVwATzWy8mYWBhcDSxAZmdrqZWfz9LCACfJTqYv1keuVwbqyp4uE3\ntlO3r8XrckTEZ/oNd+dcFLgDWAZsAB51zq0zs0Vmtije7HPAWjNbTezKmhudzhb266+vmExOOMi3\nnlqvk6siklLmVajU1NS42tpaT757KHn4jW1866n1PHDrbD49rdzrckRkiDOzlc65mv7a6Q5Vj91y\n7lgmlOTxnWc30tWtB3uISGoo3D2WFQxw5/ypbG1qZfGKnf3/ARGRJCjch4DLppYyZ/wI7vndZg53\nRL0uR0R8QOE+BJgZ/+fKqTQd7uSnr3zgdTki4gMK9yFiZtVw/nj6KP7tta3sPdTudTkikuYU7kPI\n16+YQneP4/vLNnldioikOYX7EDJmZC5/csF4HltVz/v1zV6XIyJpTOE+xNxxyemMyA3z7ad1Y5OI\nnDyF+xBTmJ3FX316Mu9s38+zaxu8LkdE0pTCfQi68ewqppQX8I/PbKC9q9vrckQkDSnch6BgwPjm\nZ6qpP9DGT1/Z6nU5IpKGFO5D1PkTivnMjNH86MUtOrkqIidM4T6E/cOCMygpiPDVX79LW6eGZ0Qk\neQr3IWxYbhb/fP0Mtja28o/PbPC6HBFJIwr3Ie7804v5nxeN5xdv7eCF9Xu9LkdE0oTCPQ389RWT\nmTa6kL/4z1UsW6fLI0Wkfwr3NBAJBXnkT8+helQhf/bISh6t1dTAInJ8Cvc0UZQX5pdfOocLTi/m\n60vW8MPlW+ju0R2sItK3pMLdzOaZ2SYzqzOzO/vYfrOZrTGz983sTTObkfpSJS8S4mdfOJtrZo7m\n7hc2c/ODb7H7YJvXZYnIENRvuJtZkNhDr+cD1cBNZlbdq9k2YK5z7kzg28ADqS5UYsKhAP9y40y+\nd9101tQ3M/+e1zQOLyJ/IJkj9zlAnXNuq3OuE1gMLEhs4Jx70zl3IL74FlCZ2jIlkZlxQ00Vv/3K\nRYwdmcufPbKSJSvrvS5LRIaQZMK9Akg8g1cfX3csfwo8eypFSXLGF+fx69vP4/wJxfzNkvf41Tsf\nel2SiAwRKT2hamafIhbuf3uM7bebWa2Z1TY2NqbyqzNWTjjIg1+o4eJJJdz1+Pv84r+3e12SiAwB\nyYT7LqAqYbkyvu4TzGw68CCwwDn3UV8f5Jx7wDlX45yrKSkpOZl6pQ/ZWUHuv3U2l00t45tL1/HS\nxn1elyQiHksm3FcAE81svJmFgYXA0sQGZjYGeBy41Tm3OfVlSn8ioSA/uuksppYX8pXF77K9qdXr\nkkTEQ/2Gu3MuCtwBLAM2AI8659aZ2SIzWxRv9k1gJHCfma02s9oBq1iOKScc5Ke3ziYYMG7/RS2H\nO6JelyQiHjGvHuVWU1Pjamu1DxgIr29p4raH3uaKaeXcd/MszMzrkkQkRcxspXOupr92ukPVhy6c\nWMxd86fy7NoGfvqqHvYhkokU7j71pYvGc9X0UXzvuY28UdfkdTkiMsgU7j5lZnzvc9M5vTSfL//q\nXXZpmgKRjKJw97G8SIj7b5lNV7SHRb9YSUt7l9clicggUbj73Gkl+fzrwpls2HOI2x56h0MKeJGM\noHDPAJdOLeO+m2exdlcztzz4Ns1HFPAifqdwzxCfnlbO/bfMZuOeFhb+21ts2dvidUkiMoAU7hnk\n0qllPHDbbHYfbGP+Pa/x3ec20tbZ7XVZIjIAFO4Z5uLJpbz4V3O55qwKfvLyB1x29yssfudDOqM9\nXpcmIimkcM9AI/Mj/OD6GTz6v85jZH6YOx9/n0/94GV+8dYOot0KeRE/ULhnsDnjR/DkX1zAw39y\nNmWFEf7uibVcc98brN99yOvSROQUKdwznJnxqcmlPPZn53PfzbNoaG7n6h+/zt0vbKZLR/EiaUvh\nLkAs5K88cxQvfG0un5kxmh8u38KtP3ubA62dXpcmIidB4S6fUJQX5l9unMndN8xg1Y6DLLj3DV02\nKZKGFO7Sp8/OquRXt5/Lkc5urr3vTX63fq/XJYnICVC4yzHNHlvE0jsuYFxxLl/6j1p+sGwT3T3e\nzP8vIidG4S7HNXp4DksWnc8NNZX8+KU6/sfD77DvULvXZYlIPxTu0q/srCDfu24G3/nsmby9bT8X\nfe8lvv30ehpbOrwuTUSOIeR1AZI+Fs4Zw7mnjeSHL27h4Te28cu3dzD/jFFcPLmEuZNKGJ4b9rpE\nEYlL6hmqZjYPuAcIAg86577Ta/sU4GFgFvAN59wP+vtMPUM1vW1rauX+lz/g+fUNHDjSRcDgvAkj\nuW52JVdMKyc3rOMGkYGQ7DNU+w13MwsCm4HLgXpgBXCTc259QptSYCxwDXBA4Z45unsca+oP8uLG\nfTyxehc797eRFw5y49lj+PIlp1OUp6N5kVRKNtyTObyaA9Q557bGP3gxsAA4Gu7OuX3APjO76iTr\nlTQVDBhnjSnirDFFfO2ySazYvp9fr9jJz9/cxpKVO/nKpRO59byxREJBr0sVySjJnFCtAHYmLNfH\n150wM7vdzGrNrLaxsfFkPkKGsEDAOOe0kdx940ye/eofMXNMEf/w2w1cfverPL1mN8kMAYpIagzq\n1TLOuQecczXOuZqSkpLB/GoZZJPLC/iPL87h3784h9xwkDv+812uve9NXt3cqGvlRQZBMsMyu4Cq\nhOXK+DqRfs2dVMKFpxfz2Mp6/vmFTdz20DsU50eYd0YZl1eXc2bFMEZoXF4k5ZIJ9xXARDMbTyzU\nFwKfH9CqxFeCAeOGs6u4euZolm/YxzPv72HJynoeeetDACqG5zC9chhzJ5Vw8eRSyodle1yxSPrr\nN9ydc1EzuwNYRuxSyIecc+vMbFF8+/1mVg7UAoVAj5n9JVDtnNPE4HJUdlaQq6aP4qrpozjSGWXV\njoOs293Mut2HWLF9P8+ubQCgelQhC+dUce1ZFRRkZ3lctUh6Suo694GgSyElkXOOzXsP8/KmfTy9\nZg/v72omNxxkwcwKrj2rgpqxRQQC5nWZIp5L2XXuA0XhLsfz3s6DPPLWDp5as5v2rh7KC7OZd0Y5\nM6qGMbG0gNNL88nO0uWVknkU7uILrR1Rlm/cx1Pv7eaVTY10xp8OZQZVRblMKMljYlkBc8aN4LwJ\nI8mL6M5Y8TeFu/hOV3cP25ta2bz3MJv3tvBB42Hq9h1ma1MrndEesoLG2eNGHD0xO6ksHzMN5Yi/\nKNwlY3REu6ndfoBXNzfyyuZGNjbEnhw1elg2Z1YO47SSfMYX51FZlMPoYTmUD8vWkI6kLYW7ZKw9\nzW28sqmRV7c0sqmhhQ/3H6Gr+5P/ziuG53BmxTCmVw1jRuVwplcO05U5khYU7iJx0e4e6g+0sftg\nG3ua29nT3MbGhhbe39XMjo+OALEx/EmlBUwdVcCo4TmMHpZN+bAcSgoiFOeHKSmIaH4cGRJSOXGY\nSFoLBQOMK85jXHHeH2w7eKST9+qbeffDA7z74UFqdxygYc0eor2mSAgYjCvOY1JpAZPK8plQms+E\nknzGFeeRFw5qbF+GHIW7ZLThuWHmToo9bORj3T2OpsMdNDS303S4g6bDHdQfaGPz3hY2723h+fUN\nJGZ/JBSgKDdMUV7sCL8kP0JZYYQxI3JjO5WReZQURAjqOn0ZRAp3kV6CAaOsMJuywr6nQeiIdrPj\noyN8sO8wO/Yf4UBrJ/vjr6bDHdTtbWFfS8cnjv4DBsX5EUoLIwzLyaIwO/YqLghTXhgbAiovzKas\nMMLIfO0I5NQp3EVOUCQUZFJZAZPKCo7ZprvHsftgG9uaWtnxUSt7D3Wwr6WdxpYOmtu62Hco9vOj\n1s4/mCUzGDCG5WRRkB0iPxJiWE5W/DeDLEbkRSgtiFBSEGFkXpiC7Czy4+0KIiHdxStHKdxFBkAw\nYFSNyKVqRC5w7OmtE4eA9h6KvRoOtdPc1kVLe5SW9iiH2rrY0HCIA62dHGzr4ljXQJhBfjhEYU4W\nI/LCjMgLU5SbRUF2FnmREAXZIfLCwaPvC3OyGJ4TZnhubAeRmxUkFBzUWcBlACncRTzU3xBQb13d\nPexv7aSxpYOPWjtp7YjS0h7bERxqj71vPtLF/iOxYaKtTYc53B7lcEf0Dy4H7UskFCA/EiI3EiQv\nHPuNoDD+W0ReJEROVpDccJCccGx7TjhIfiS2LfYzSHYotj073jZLOwxPKNxF0khWMHBCO4NE7V3d\ntHZEae3opqWji+a22I6gua2Lwx1RjnTGtn/883D81djSwQeNh2nt6KatM8qRru5j/vbQd81GbjhE\nbvj3O4bsUCz8s7MC5IRD5GQFyA2Hjq7LzgqSHYr/jL9ywkFysoJEQgEiWQEiodj7nPj2SCigYakE\nCneRDPFxSI7MP7XPcc7R3tXDkc74jqAzyuH2KC0dUdo6u2nv6qatq5u2ztjrSFc3R+I7jdgrSke0\nh7aubva3dtLW1X30szq6eo7OH3QywsHA0fAPBwNEju4M4j/jr3Do9zuHcMK6cDAY+5mwPhKKfdbH\n67I+ft/rZ1Yw9j4rZISDAc+HuBTuInJCzCx2FB0OMnIAPr+7x9He1U1HtOfojqI9/mrr7KEjGtvW\nEY3tDNq7ummP9sTeR2PtOqM9R/98Z7SH9vj7lvYoH0VjO5CO6O/bfbxTSeUjIAPGJ0M/HvxZwQCf\nnzOGL110Wsq+qy8KdxEZUoIBIy8SIi8y+N/d3ePojPbEQz+2E+ns7jm6riv+vqO7h66EbV3dPXR2\nu0+0+f362M+uqDu6XFIw8J1TuIuIxAUDv/+tBNJ7riGdxhYR8aGkwt3M5pnZJjOrM7M7+9huZvbD\n+PY1ZjYr9aWKiEiy+g13MwsC9wLzgWrgJjOr7tVsPjAx/rod+EmK6xQRkROQzJH7HKDOObfVOdcJ\nLAYW9GqzAPgPF/MWMNzMRqW4VhERSVIy4V4B7ExYro+vO9E2mNntZlZrZrWNjY0nWquIiCRpUE+o\nOucecM7VOOdqSkqOPd+GiIicmmTCfRdQlbBcGV93om1ERGSQJBPuK4CJZjbezMLAQmBprzZLgdvi\nV82cCzQ75/akuFYREUlSvzcxOeeiZnYHsAwIAg8559aZ2aL49vuBZ4ArgTrgCPAn/X3uypUrm8xs\nx0nWXQw0neSfTWeZ2O9M7DNkZr8zsc9w4v0em0wjzx6QfSrMrDaZB8T6TSb2OxP7DJnZ70zsMwxc\nv3WHqoiIDyncRUR8KF3D/QGvC/BIJvY7E/sMmdnvTOwzDFC/03LMXUREji9dj9xFROQ40i7c+5uh\n0g/MrMrMXjKz9Wa2zsy+Gl8/wsxeMLMt8Z9FXteaamYWNLN3zezp+HIm9Hm4mS0xs41mtsHMzsuQ\nfn8t/u97rZn9ysyy/dZvM3vIzPaZ2dqEdcfso5ndFc+2TWZ2xal8d1qFe5IzVPpBFPgr51w1cC7w\nF/F+3gksd85NBJbHl/3mq8CGhOVM6PM9wHPOuSnADGL993W/zawC+ApQ45w7g9g9NAvxX79/Dszr\nta7PPsb/jy8EpsX/zH3xzDspaRXuJDdDZdpzzu1xzq2Kv28h9p+9glhf/z3e7N+Ba7ypcGCYWSVw\nFfBgwmq/93kY8EfAzwCcc53OuYP4vN9xISDHzEJALrAbn/XbOfcqsL/X6mP1cQGw2DnX4ZzbRuym\n0Dkn+93pFu5JzT7pJ2Y2DjgLeBsoS5jWoQEo86isgfKvwNeBnoR1fu/zeKAReDg+HPWgmeXh8347\n53YBPwA+BPYQm7LkeXze77hj9TGl+ZZu4Z5RzCwfeAz4S+fcocRtLnaZk28udTKzPwb2OedWHquN\n3/ocFwKFwDTKAAABf0lEQVRmAT9xzp0FtNJrKMKP/Y6PMy8gtnMbDeSZ2S2JbfzY794Gso/pFu4Z\nM/ukmWURC/ZfOucej6/e+/FDUOI/93lV3wC4ALjazLYTG267xMwewd99htjRWb1z7u348hJiYe/3\nfl8GbHPONTrnuoDHgfPxf7/h2H1Mab6lW7gnM0Nl2jMzIzYGu8E5d3fCpqXAF+LvvwA8Odi1DRTn\n3F3OuUrn3Dhif68vOuduwcd9BnDONQA7zWxyfNWlwHp83m9iwzHnmllu/N/7pcTOLfm933DsPi4F\nFppZxMzGE3ts6Tsn/S3OubR6EZt9cjPwAfANr+sZoD5eSOxXtTXA6vjrSmAksbPrW4DfASO8rnWA\n+n8x8HT8ve/7DMwEauN/308ARRnS728BG4G1wC+AiN/6DfyK2DmFLmK/pf3p8foIfCOebZuA+afy\n3bpDVUTEh9JtWEZERJKgcBcR8SGFu4iIDyncRUR8SOEuIuJDCncRER9SuIuI+JDCXUTEh/4/6/RV\nOJe/zY0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37320ec908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 39us/step\n",
      "Loss = 0.029424718648195267, accuracy = 0.9895\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_val, Y_val)\n",
    "print(f'Loss = {loss}, accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2365265 -6567759] -6567759\n",
      "[ 8765610 -1074881] -1074881\n",
      "[1201293 1416632] 1201293\n",
      "[3469059 7881131] 3469059\n",
      "[-3338698  -405794] -3338698\n",
      "[-3479924 -5507460] -5507460\n",
      "[-1610653  4895259] -1610653\n",
      "[-1070131  5527263] -1070131\n",
      "[ 5226025 -6211838] -6211838\n",
      "[-3031653  9675734] -3031653\n"
     ]
    }
   ],
   "source": [
    "# Check if the model generalizes well with bigger numbers\n",
    "X_test, _ = create_dataset(10, 10000000)\n",
    "for inp in X_test:\n",
    "    print(inp, inp[np.argmax(model.predict(np.expand_dims(inp, axis=0)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
